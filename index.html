<!DOCTYPE html>
<html lang="en" class="h-full bg-gray-900 text-gray-100">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Live DQN Maze Learner</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <style>
        body { font-family: 'Inter', sans-serif; }
        .grid-cell {
            width: 35px;
            height: 35px;
            border: 1px solid #4a5568;
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 1rem;
            cursor: pointer;
            user-select: none;
            transition: background-color 0.1s ease;
        }
        /* Cell Types */
        .cell-free { background-color: #2d3748; } /* gray-800 */
        .cell-obstacle { background-color: #c53030; } /* red-700 */
        .cell-start { background-color: #2b6cb0; } /* blue-700 */
        .cell-goal { background-color: #2f855a; } /* green-700 */
        .cell-path { background-color: #d69e2e; } /* yellow-600 */
        
        /* Heatmap colors */
        .heatmap-cell {
            width: 35px;
            height: 35px;
            border: 1px solid #4a5568;
            opacity: 0.9;
        }

        /* Button styles */
        .btn {
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            font-weight: 600;
            transition: background-color 0.2s;
            cursor: pointer;
        }
        .btn-green { background-color: #38a169; color: white; }
        .btn-green:hover { background-color: #2f855a; }
        .btn:disabled { background-color: #4a5568; cursor: not-allowed; }
    </style>
</head>
<body class="h-full flex flex-col items-center justify-center p-4">

    <h1 class="text-3xl font-bold mb-4">Live DQN Maze Learner</h1>
    <p classD="text-gray-400 mb-4 text-center">Click cells to toggle obstacles (游린). Click again to set Start (游댯), then Goal (游릭). Then press Start Training.</p>

    <!-- Main Content Area -->
    <div class="flex flex-col md:flex-row gap-8">
        
        <!-- Left Panel: Agent's Best Path -->
        <div class="flex flex-col items-center">
            <h2 class="text-xl font-semibold mb-2" id="left-panel-title">Agent's Current Path</h2>
            <div id="grid-container" class="grid grid-cols-10 border border-gray-600 rounded-lg overflow-hidden shadow-lg">
                <!-- Grid cells will be generated by JS -->
            </div>
            <div class="mt-2 text-sm text-gray-400" id="path-status">Setup the grid.</div>
        </div>

        <!-- Right Panel: Q-Value Heatmap -->
        <div class="flex flex-col items-center">
            <h2 class="text-xl font-semibold mb-2">NN Q-Value Heatmap</h2>
            <div id="heatmap-container" class="grid grid-cols-10 border border-gray-600 rounded-lg overflow-hidden shadow-lg">
                <!-- Heatmap cells will be generated by JS -->
            </div>
            <div class="mt-2 text-sm text-gray-400" id="model-status">Model is idle.</div>
        </div>

    </div>
    
    <button id="start-button" class="btn btn-green mt-6">Start Training</button>

    <!-- Stats Panel -->
    <div class="mt-6 p-4 bg-gray-800 rounded-lg shadow-inner w-full max-w-4xl">
        <h3 class="text-lg font-semibold mb-2">Training Status</h3>
        <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-gray-300">
            <div>Episode: <span id="episode-value" class="font-mono">0</span></div>
            <div>Loss: <span id="loss-value" class="font-mono">N/A</span></div>
            <div>Epsilon: <span id="epsilon-value" class="font-mono">1.00</span></div>
            <div>Replay Samples: <span id="samples-value" class="font-mono">0</span></div>
        </div>
    </div>

    <!-- JavaScript Logic -->
    <script type="module">
        // --- Constants & State ---
        const GRID_SIZE = 10;
        const NUM_STATES = GRID_SIZE * GRID_SIZE; // 100
        const NUM_ACTIONS = 4; // 0:Up, 1:Down, 2:Left, 3:Right
        
        const CELL_FREE = 0;
        const CELL_OBSTACLE = 1;
        const CELL_START = 2;
        const CELL_GOAL = 3;
        
        // Grid state
        let grid = [];
        let startPos = null; // {r, c}
        let goalPos = null; // {r, c}
        let setupPhase = 'start'; // 'start' -> 'goal' -> 'obstacles'
        
        // DOM elements
        const gridContainer = document.getElementById('grid-container');
        const heatmapContainer = document.getElementById('heatmap-container');
        const pathStatus = document.getElementById('path-status');
        const modelStatus = document.getElementById('model-status');
        const startButton = document.getElementById('start-button');
        const leftPanelTitle = document.getElementById('left-panel-title');
        
        const episodeValue = document.getElementById('episode-value');
        const lossValue = document.getElementById('loss-value');
        const epsilonValue = document.getElementById('epsilon-value');
        const samplesValue = document.getElementById('samples-value');

        // --- RL & NN State ---
        let qNet, targetNet;
        let replayBuffer = [];
        const MAX_REPLAY_BUFFER = 10000;
        const BATCH_SIZE = 64;
        
        // Hyperparameters
        let epsilon = 1.0;
        const EPSILON_DECAY = 0.999;
        const MIN_EPSILON = 0.01;
        const GAMMA = 0.95; // Discount factor
        const LEARNING_RATE = 0.001;
        const TARGET_UPDATE_EVERY = 5; // episodes
        
        let isTraining = false;

        // --- Best Path Tracking ---
        let bestPathFound = [];
        let bestPathLength = Infinity;

        // --- 1. Grid & Interaction Logic ---
        
        /** Initializes the grid state and DOM */
        function initGrid() {
            gridContainer.innerHTML = '';
            heatmapContainer.innerHTML = '';
            grid = Array(GRID_SIZE).fill(null).map(() => Array(GRID_SIZE).fill(CELL_FREE));
            
            for (let r = 0; r < GRID_SIZE; r++) {
                for (let c = 0; c < GRID_SIZE; c++) {
                    // Left Grid
                    const cell = document.createElement('div');
                    cell.className = 'grid-cell cell-free';
                    cell.dataset.r = r;
                    cell.dataset.c = c;
                    cell.id = `cell-${r}-${c}`;
                    gridContainer.appendChild(cell);
                    
                    // Right Grid
                    const mapCell = document.createElement('div');
                    mapCell.className = 'heatmap-cell';
                    mapCell.id = `heat-${r}-${c}`;
                    mapCell.style.backgroundColor = 'rgb(45, 55, 72)'; // gray-800
                    heatmapContainer.appendChild(mapCell);
                }
            }
            gridContainer.addEventListener('click', handleGridClick);
            pathStatus.textContent = 'Click to set Start (游댯)';
        }

        /** Handles clicking on the grid to set up the maze */
        function handleGridClick(e) {
            if (isTraining) return; // Don't allow edits while training
            
            const cell = e.target.closest('.grid-cell');
            if (!cell) return;
            
            const r = parseInt(cell.dataset.r);
            const c = parseInt(cell.dataset.c);

            if (setupPhase === 'start') {
                startPos = { r, c };
                grid[r][c] = CELL_START;
                cell.classList.add('cell-start');
                cell.innerHTML = '游댯';
                setupPhase = 'goal';
                pathStatus.textContent = 'Click to set Goal (游릭)';
            } else if (setupPhase === 'goal') {
                if (r === startPos.r && c === startPos.c) return; // Can't be same as start
                goalPos = { r, c };
                grid[r][c] = CELL_GOAL;
                cell.classList.add('cell-goal');
                cell.innerHTML = '游릭';
                setupPhase = 'obstacles';
                pathStatus.textContent = 'Click to toggle obstacles (游린)';
            } else {
                // Toggle obstacles
                if (r === startPos.r && c === startPos.c) return;
                if (r === goalPos.r && c === goalPos.c) return;
                
                if (grid[r][c] === CELL_FREE) {
                    grid[r][c] = CELL_OBSTACLE;
                    cell.classList.add('cell-obstacle');
                } else {
                    grid[r][c] = CELL_FREE;
                    cell.classList.remove('cell-obstacle');
                }
            }
        }
        
        /** Converts (r, c) to a state index (0-99) */
        function posToState(r, c) {
            return r * GRID_SIZE + c;
        }

        /** Converts a state index (0-99) to (r, c) */
        function stateToPos(state) {
            return { r: Math.floor(state / GRID_SIZE), c: state % GRID_SIZE };
        }

        // --- 2. DQN (Agent) Logic ---

        /** Creates the Q-Network model */
        function createDQNModel() {
            const model = tf.sequential();
            
            // Input is a one-hot vector of size 100
            model.add(tf.layers.dense({
                inputShape: [NUM_STATES],
                units: 64,
                activation: 'relu'
            }));
            model.add(tf.layers.dense({
                units: 32,
                activation: 'relu'
            }));
            // Output is 4 Q-values, one for each action
            model.add(tf.layers.dense({
                units: NUM_ACTIONS,
                activation: 'linear' // No activation, we want raw Q-values
            }));
            
            model.compile({
                optimizer: tf.train.adam(LEARNING_RATE),
                loss: 'meanSquaredError'
            });
            
            return model;
        }

        /** Gets the best action from the Q-network (exploitation) or a random one (exploration) */
        function getAction(state, useEpsilon = true) {
            if (useEpsilon && Math.random() < epsilon) {
                return Math.floor(Math.random() * NUM_ACTIONS); // Explore
            }
            
            // Exploit
            return tf.tidy(() => {
                const qValues = qNet.predict(tf.oneHot(tf.tensor1d([state], 'int32'), NUM_STATES));
                return qValues.argMax(1).dataSync()[0];
            });
        }
        
        /**
         * Simulates one step in the environment
         * @returns { {nextState: number, reward: number, terminated: boolean} }
         */
        function step(state, action) {
            let { r, c } = stateToPos(state);

            // 0:Up, 1:Down, 2:Left, 3:Right
            if (action === 0) r = Math.max(0, r - 1);
            else if (action === 1) r = Math.min(GRID_SIZE - 1, r + 1);
            else if (action === 2) c = Math.max(0, c - 1);
            else if (action === 3) c = Math.min(GRID_SIZE - 1, c + 1);

            const nextState = posToState(r, c);
            const cellType = grid[r][c];

            let reward = -0.01; // Small penalty for existing
            let terminated = false;

            if (cellType === CELL_GOAL) {
                reward = 1.0; // Big reward for goal
                terminated = true;
            } else if (cellType === CELL_OBSTACLE) {
                reward = -0.5; // Big penalty for obstacle
                // Stay in the same state (bounce back) - this is a simple model
                return { nextState: state, reward, terminated: false };
            }
            
            return { nextState, reward, terminated };
        }
        
        /** Adds a transition to the replay buffer */
        function addToBuffer(state, action, reward, nextState, terminated) {
            if (replayBuffer.length >= MAX_REPLAY_BUFFER) {
                replayBuffer.shift(); // Remove oldest
            }
            replayBuffer.push({ state, action, reward, nextState, terminated });
            samplesValue.textContent = replayBuffer.length;
        }
        
        /** Trains the model on a batch of replay data */
        async function trainBatch() {
            if (replayBuffer.length < BATCH_SIZE) {
                return; // Not enough samples to train
            }
            
            // Sample a batch
            const batch = [];
            for (let i = 0; i < BATCH_SIZE; i++) {
                batch.push(replayBuffer[Math.floor(Math.random() * replayBuffer.length)]);
            }

            const { states, actions, rewards, nextStates, terminateds } = tf.tidy(() => {
                const states = batch.map(t => t.state);
                const actions = batch.map(t => t.action);
                const rewards = batch.map(t => t.reward);
                const nextStates = batch.map(t => t.nextState);
                const terminateds = batch.map(t => t.terminated);
                return { states, actions, rewards, nextStates, terminateds };
            });

            // Calculate target Q-values (Bellman equation)
            const targets = tf.tidy(() => {
                const nextQValues = targetNet.predict(tf.oneHot(tf.tensor1d(nextStates, 'int32'), NUM_STATES));
                const maxNextQ = nextQValues.max(1); // Max Q-value for next state
                
                // target = reward + GAMMA * maxNextQ
                // If terminated, target is just the reward
                const targetQ = tf.tensor1d(rewards).add(
                    tf.tensor1d(terminateds).logicalNot().cast('float32').mul(GAMMA).mul(maxNextQ)
                );
                
                // Get current Q-values to update only the action taken
                const currentQValues = qNet.predict(tf.oneHot(tf.tensor1d(states, 'int32'), NUM_STATES));
                
                const buffer = currentQValues.bufferSync(); // Synchronous
                const targetQData = targetQ.dataSync();   // Synchronous

                for(let i = 0; i < BATCH_SIZE; i++) {
                    buffer.set(targetQData[i], i, actions[i]);
                }
                
                // Manually dispose of the intermediate tensor since we called .dataSync()
                targetQ.dispose(); 
                
                return buffer.toTensor();
            }); 

            // Train the network
            const history = await qNet.fit(
                tf.oneHot(tf.tensor1d(states, 'int32'), NUM_STATES),
                targets,
                { epochs: 1, batchSize: BATCH_SIZE, verbose: 0 }
            );
            
            lossValue.textContent = history.history.loss[0].toFixed(4);
            
            // Clean up
            tf.dispose([targets]);
        }
        
        // --- 3. Visualization Logic ---

        /** Runs a greedy episode, updates best path, and draws the CURRENT path on the left grid */
        function updateLeftPanel() {
            // Clear previous path drawing
            document.querySelectorAll('.grid-cell.cell-path').forEach(c => c.classList.remove('cell-path'));
            
            // --- 1. Calculate Current Greedy Path ---
            let state = posToState(startPos.r, startPos.c);
            let currentGreedyPath = [state]; // Start with the start position
            let terminated = false;
            let steps = 0;
            const maxSteps = 50; // Prevent infinite loops

            while (!terminated && steps < maxSteps) {
                const action = getAction(state, false); // Greedy (no epsilon)
                const { nextState, reward, terminated: term } = step(state, action);
                
                state = nextState;
                currentGreedyPath.push(state); // Add the new state to the path
                terminated = term;
                steps++;
            }

            // --- 2. Update Best Path if necessary (silently) ---
            if (terminated) {
                // Agent found the goal on this greedy run
                if (steps < bestPathLength) {
                    bestPathLength = steps;
                    bestPathFound = currentGreedyPath; // Save this path
                    pathStatus.textContent = `New Best Path! Found goal in ${steps} steps.`;
                } else {
                    pathStatus.textContent = `Found goal in ${steps} steps. (Best: ${bestPathLength} steps)`;
                }
            } else {
                // Agent failed to find the goal
                const bestStatus = bestPathLength === Infinity ? 'N/A' : `${bestPathLength} steps`;
                pathStatus.textContent = `Agent failed to find goal. (Best: ${bestStatus})`;
            }

            // --- 3. Draw the CURRENT Greedy Path ---
            for (const s of currentGreedyPath) {
                // Don't draw over start or goal cells
                if (s === posToState(startPos.r, startPos.c) || s === posToState(goalPos.r, goalPos.c)) {
                    continue;
                }
                
                const { r, c } = stateToPos(s);
                const cellElement = document.getElementById(`cell-${r}-${c}`);
                // Only draw on free cells
                if (cellElement && !cellElement.classList.contains('cell-obstacle')) {
                    cellElement.classList.add('cell-path');
                }
            }
        }
        
        /** Calculates Q-values for all cells and draws the heatmap on the right */
        function updateRightPanel() {
            tf.tidy(() => {
                const allStates = tf.tensor1d(Array.from({length: NUM_STATES}, (_, i) => i), 'int32');
                const allQValues = qNet.predict(tf.oneHot(allStates, NUM_STATES));
                const maxQValues = allQValues.max(1).dataSync(); // [100]
                
                // Normalize for better visualization (find min/max)
                let minQ = Infinity;
                let maxQ = -Infinity;
                for (const q of maxQValues) {
                    if (q > maxQ) maxQ = q;
                    if (q < minQ) minQ = q;
                }
                
                const range = maxQ - minQ;
                if (range === 0) return; // Avoid division by zero
                
                for (let s = 0; s < NUM_STATES; s++) {
                    const { r, c } = stateToPos(s);
                    const cell = document.getElementById(`heat-${r}-${c}`);
                    
                    if (grid[r][c] === CELL_OBSTACLE) {
                        cell.style.backgroundColor = 'rgb(197, 48, 48)'; // red-700
                    } else if (grid[r][c] === CELL_GOAL) {
                        cell.style.backgroundColor = 'rgb(47, 133, 90)'; // green-700
                    } else {
                        // Normalize Q-value from 0 to 1
                        const normQ = (maxQValues[s] - minQ) / range;
                        const intensity = Math.floor(normQ * 255);
                        // Heatmap (black -> red -> yellow)
                        const rC = intensity;
                        const gC = Math.max(0, Math.floor((normQ - 0.5) * 2 * 255));
                        cell.style.backgroundColor = `rgb(${rC}, ${gC}, 0)`;
                    }
                }
            });
        }
        
        // --- 4. Main Training Loop ---
        
        async function startTraining() {
            if (!startPos || !goalPos) {
                pathStatus.textContent = 'Please set Start and Goal first!';
                return;
            }
            if (isTraining) return;
            
            isTraining = true;
            startButton.disabled = true;
            startButton.textContent = 'Training...';
            gridContainer.removeEventListener('click', handleGridClick);
            leftPanelTitle.textContent = "Agent's Current Path";
            
            // Create models
            qNet = createDQNModel();
            targetNet = createDQNModel();
            targetNet.setWeights(qNet.getWeights());
            
            // Reset best path tracking
            bestPathFound = [];
            bestPathLength = Infinity;
            
            modelStatus.textContent = 'Training...';
            
            const numEpisodes = 2000;
            for (let i = 0; i < numEpisodes; i++) {
                let state = posToState(startPos.r, startPos.c);
                let terminated = false;
                let episodeSteps = 0;
                const maxEpisodeSteps = 100;

                while (!terminated && episodeSteps < maxEpisodeSteps) {
                    const action = getAction(state, true); // Epsilon-greedy
                    const { nextState, reward, terminated: term } = step(state, action);
                    
                    addToBuffer(state, action, reward, nextState, term);
                    
                    state = nextState;
                    terminated = term;
                    episodeSteps++;
                }

                // Train on a batch
                await trainBatch();
                
                // Update stats and visuals periodically
                if (i % TARGET_UPDATE_EVERY === 0) {
                    targetNet.setWeights(qNet.getWeights()); // Update target network
                    
                    // Update UI
                    episodeValue.textContent = i;
                    epsilon = Math.max(MIN_EPSILON, epsilon * EPSILON_DECAY);
                    epsilonValue.textContent = epsilon.toFixed(3);
                    
                    // Update the two panels
                    updateLeftPanel(); // This now tracks and draws the CURRENT path
                    updateRightPanel();
                    
                    // Allow UI to refresh
                    await new Promise(r => setTimeout(r, 0));
                }
            }
            
            modelStatus.textContent = 'Training Complete!';
            leftPanelTitle.textContent = "Agent's Best Path Found";
            
            // --- FINAL PATH DRAW ---
            // Clear the last "current" path from the display
            document.querySelectorAll('.grid-cell.cell-path').forEach(c => c.classList.remove('cell-path'));

            // Draw the final BEST path
            if (bestPathLength !== Infinity) {
                for (const s of bestPathFound) {
                    if (s === posToState(startPos.r, startPos.c) || s === posToState(goalPos.r, goalPos.c)) {
                        continue;
                    }
                    const { r, c } = stateToPos(s);
                    const cellElement = document.getElementById(`cell-${r}-${c}`);
                    if (cellElement && !cellElement.classList.contains('cell-obstacle')) {
                        cellElement.classList.add('cell-path');
                    }
                }
                 // Update status to reflect the final best path
                pathStatus.textContent = `Training complete. Best path found: ${bestPathLength} steps.`;
            } else {
                // Agent never found the goal
                pathStatus.textContent = `Training complete. Agent never found the goal.`;
            }
            // --- END FINAL PATH DRAW ---
            
            startButton.disabled = false;
            startButton.textContent = 'Restart Training';
            gridContainer.addEventListener('click', handleGridClick);
            isTraining = false;
        }
        
        // --- 5. Initialization ---
        initGrid();
        startButton.addEventListener('click', startTraining);

    </script>
</body>
</html>